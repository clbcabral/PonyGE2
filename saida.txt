(ge) cleber@dell:~/workspace/PonyGE2/src$ python3 ponyge.py --parameters paper.txt

Start:   2020-11-01 22:41:32.288034 

Warning: Grammar contains unit production for production rule <FINAL_EXP>
         Unit productions consume GE codons.
Warning: Grammar contains unit production for production rule <EXP_2>
         Unit productions consume GE codons.
Warning: Grammar contains unit production for production rule <EXP_1>
         Unit productions consume GE codons.
Warning: Grammar contains unit production for production rule <FC>
         Unit productions consume GE codons.
Warning: Grammar contains unit production for production rule <CONV>
         Unit productions consume GE codons.

FENOTIPO: (((conv*1)pool)*1)fc*1

2020-11-01 22:41:35.780292: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-11-01 22:41:35.805755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2195070000 Hz
2020-11-01 22:41:35.806726: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ba63af0870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-01 22:41:35.806797: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-01 22:41:35.807018: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
_________________________________________________________________
dense (Dense)                (None, 15, 15, 256)       8448      
_________________________________________________________________
flatten (Flatten)            (None, 57600)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                576010    
=================================================================
Total params: 585,354
Trainable params: 585,354
Non-trainable params: 0
_________________________________________________________________
Epoch 1/70
1563/1563 [==============================] - 142s 91ms/step - loss: 1.8891 - accuracy: 0.3317 - val_loss: 1.6759 - val_accuracy: 0.4078
Epoch 2/70
1563/1563 [==============================] - 133s 85ms/step - loss: 1.5637 - accuracy: 0.4504 - val_loss: 1.4949 - val_accuracy: 0.4694
Epoch 3/70
1563/1563 [==============================] - 133s 85ms/step - loss: 1.4184 - accuracy: 0.5008 - val_loss: 1.3649 - val_accuracy: 0.5232
Epoch 4/70
1563/1563 [==============================] - 134s 86ms/step - loss: 1.3205 - accuracy: 0.5394 - val_loss: 1.2919 - val_accuracy: 0.5529
Epoch 5/70
1563/1563 [==============================] - 129s 83ms/step - loss: 1.2538 - accuracy: 0.5616 - val_loss: 1.2766 - val_accuracy: 0.5527
Epoch 6/70
1563/1563 [==============================] - 133s 85ms/step - loss: 1.2014 - accuracy: 0.5815 - val_loss: 1.2999 - val_accuracy: 0.5465
Epoch 7/70
1563/1563 [==============================] - 132s 85ms/step - loss: 1.1559 - accuracy: 0.5993 - val_loss: 1.1714 - val_accuracy: 0.5888
Epoch 8/70
1563/1563 [==============================] - 134s 86ms/step - loss: 1.1170 - accuracy: 0.6126 - val_loss: 1.1482 - val_accuracy: 0.5914
Epoch 9/70
1563/1563 [==============================] - 133s 85ms/step - loss: 1.0784 - accuracy: 0.6276 - val_loss: 1.1305 - val_accuracy: 0.5996
Epoch 10/70
1563/1563 [==============================] - 134s 86ms/step - loss: 1.0445 - accuracy: 0.6396 - val_loss: 1.1048 - val_accuracy: 0.6116
Epoch 11/70
1563/1563 [==============================] - 131s 84ms/step - loss: 1.0108 - accuracy: 0.6509 - val_loss: 1.1218 - val_accuracy: 0.6033
Epoch 12/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.9804 - accuracy: 0.6623 - val_loss: 1.0991 - val_accuracy: 0.6100
Epoch 13/70
1563/1563 [==============================] - 135s 87ms/step - loss: 0.9521 - accuracy: 0.6720 - val_loss: 1.0640 - val_accuracy: 0.6250
Epoch 14/70
1563/1563 [==============================] - 135s 87ms/step - loss: 0.9254 - accuracy: 0.6801 - val_loss: 1.0535 - val_accuracy: 0.6292
Epoch 15/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.9020 - accuracy: 0.6894 - val_loss: 1.1046 - val_accuracy: 0.6164
Epoch 16/70
1563/1563 [==============================] - 135s 86ms/step - loss: 0.8783 - accuracy: 0.6973 - val_loss: 1.0516 - val_accuracy: 0.6294
Epoch 17/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.8573 - accuracy: 0.7054 - val_loss: 1.0424 - val_accuracy: 0.6350
Epoch 18/70
1563/1563 [==============================] - 132s 85ms/step - loss: 0.8371 - accuracy: 0.7120 - val_loss: 1.0231 - val_accuracy: 0.6421
Epoch 19/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.8161 - accuracy: 0.7206 - val_loss: 1.0611 - val_accuracy: 0.6315
Epoch 20/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.7982 - accuracy: 0.7267 - val_loss: 1.0247 - val_accuracy: 0.6419
Epoch 21/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.7785 - accuracy: 0.7329 - val_loss: 1.0550 - val_accuracy: 0.6339
Epoch 22/70
1563/1563 [==============================] - 135s 86ms/step - loss: 0.7599 - accuracy: 0.7390 - val_loss: 1.0642 - val_accuracy: 0.6274
Epoch 23/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.7415 - accuracy: 0.7466 - val_loss: 1.0662 - val_accuracy: 0.6376
Epoch 24/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.7247 - accuracy: 0.7540 - val_loss: 1.0346 - val_accuracy: 0.6434
Epoch 25/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.7070 - accuracy: 0.7604 - val_loss: 1.0300 - val_accuracy: 0.6424
Epoch 26/70
1563/1563 [==============================] - 134s 85ms/step - loss: 0.6874 - accuracy: 0.7675 - val_loss: 1.0307 - val_accuracy: 0.6425
Epoch 27/70
1563/1563 [==============================] - 132s 85ms/step - loss: 0.6714 - accuracy: 0.7729 - val_loss: 1.0288 - val_accuracy: 0.6509
Epoch 28/70
1563/1563 [==============================] - 139s 89ms/step - loss: 0.6554 - accuracy: 0.7770 - val_loss: 1.0346 - val_accuracy: 0.6450
Epoch 29/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.6401 - accuracy: 0.7826 - val_loss: 1.0847 - val_accuracy: 0.6315
Epoch 30/70
1563/1563 [==============================] - 134s 85ms/step - loss: 0.6228 - accuracy: 0.7894 - val_loss: 1.0922 - val_accuracy: 0.6326
Epoch 31/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.6082 - accuracy: 0.7937 - val_loss: 1.0532 - val_accuracy: 0.6415
Epoch 32/70
1563/1563 [==============================] - 135s 87ms/step - loss: 0.5914 - accuracy: 0.8000 - val_loss: 1.0569 - val_accuracy: 0.6400
Epoch 33/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.5755 - accuracy: 0.8054 - val_loss: 1.1412 - val_accuracy: 0.6260
Epoch 34/70
1563/1563 [==============================] - 132s 84ms/step - loss: 0.5609 - accuracy: 0.8116 - val_loss: 1.1098 - val_accuracy: 0.6356
Epoch 35/70
1563/1563 [==============================] - 135s 86ms/step - loss: 0.5455 - accuracy: 0.8159 - val_loss: 1.1123 - val_accuracy: 0.6385
Epoch 36/70
1563/1563 [==============================] - 128s 82ms/step - loss: 0.5299 - accuracy: 0.8233 - val_loss: 1.0961 - val_accuracy: 0.6386
Epoch 37/70
1563/1563 [==============================] - 129s 82ms/step - loss: 0.5164 - accuracy: 0.8267 - val_loss: 1.0817 - val_accuracy: 0.6469
Epoch 38/70
1563/1563 [==============================] - 129s 82ms/step - loss: 0.5002 - accuracy: 0.8322 - val_loss: 1.1064 - val_accuracy: 0.6479
Epoch 39/70
1563/1563 [==============================] - 134s 85ms/step - loss: 0.4860 - accuracy: 0.8390 - val_loss: 1.2175 - val_accuracy: 0.6191
Epoch 40/70
1563/1563 [==============================] - 126s 81ms/step - loss: 0.4709 - accuracy: 0.8432 - val_loss: 1.1613 - val_accuracy: 0.6407
Epoch 41/70
1563/1563 [==============================] - 137s 88ms/step - loss: 0.4559 - accuracy: 0.8486 - val_loss: 1.1861 - val_accuracy: 0.6260
Epoch 42/70
1563/1563 [==============================] - 134s 86ms/step - loss: 0.4437 - accuracy: 0.8520 - val_loss: 1.1647 - val_accuracy: 0.6395
Epoch 43/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.4291 - accuracy: 0.8605 - val_loss: 1.2047 - val_accuracy: 0.6315
Epoch 44/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.4149 - accuracy: 0.8650 - val_loss: 1.2608 - val_accuracy: 0.6265
Epoch 45/70
1563/1563 [==============================] - 132s 84ms/step - loss: 0.3998 - accuracy: 0.8696 - val_loss: 1.2067 - val_accuracy: 0.6327
Epoch 46/70
1563/1563 [==============================] - 137s 88ms/step - loss: 0.3870 - accuracy: 0.8754 - val_loss: 1.2186 - val_accuracy: 0.6352
Epoch 47/70
1563/1563 [==============================] - 132s 84ms/step - loss: 0.3746 - accuracy: 0.8795 - val_loss: 1.2487 - val_accuracy: 0.6334
Epoch 48/70
1563/1563 [==============================] - 129s 83ms/step - loss: 0.3611 - accuracy: 0.8841 - val_loss: 1.2822 - val_accuracy: 0.6255
Epoch 49/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.3487 - accuracy: 0.8873 - val_loss: 1.3059 - val_accuracy: 0.6295
Epoch 50/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.3351 - accuracy: 0.8935 - val_loss: 1.2801 - val_accuracy: 0.6390
Epoch 51/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.3207 - accuracy: 0.8997 - val_loss: 1.3082 - val_accuracy: 0.6380
Epoch 52/70
1563/1563 [==============================] - 132s 85ms/step - loss: 0.3096 - accuracy: 0.9022 - val_loss: 1.3175 - val_accuracy: 0.6326
Epoch 53/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.2975 - accuracy: 0.9079 - val_loss: 1.3041 - val_accuracy: 0.6386
Epoch 54/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.2853 - accuracy: 0.9122 - val_loss: 1.3391 - val_accuracy: 0.6366
Epoch 55/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.2728 - accuracy: 0.9173 - val_loss: 1.3848 - val_accuracy: 0.6319
Epoch 56/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.2618 - accuracy: 0.9210 - val_loss: 1.3979 - val_accuracy: 0.6360
Epoch 57/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.2512 - accuracy: 0.9246 - val_loss: 1.3949 - val_accuracy: 0.6338
Epoch 58/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.2392 - accuracy: 0.9286 - val_loss: 1.4064 - val_accuracy: 0.6369
Epoch 59/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.2303 - accuracy: 0.9323 - val_loss: 1.4781 - val_accuracy: 0.6265
Epoch 60/70
1563/1563 [==============================] - 132s 84ms/step - loss: 0.2185 - accuracy: 0.9367 - val_loss: 1.4698 - val_accuracy: 0.6320
Epoch 61/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.2100 - accuracy: 0.9397 - val_loss: 1.4502 - val_accuracy: 0.6379
Epoch 62/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.1995 - accuracy: 0.9442 - val_loss: 1.5310 - val_accuracy: 0.6237
Epoch 63/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.1894 - accuracy: 0.9485 - val_loss: 1.6933 - val_accuracy: 0.6066
Epoch 64/70
1563/1563 [==============================] - 132s 85ms/step - loss: 0.1802 - accuracy: 0.9507 - val_loss: 1.5643 - val_accuracy: 0.6305
Epoch 65/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.1686 - accuracy: 0.9570 - val_loss: 1.6870 - val_accuracy: 0.6174
Epoch 66/70
1563/1563 [==============================] - 130s 83ms/step - loss: 0.1611 - accuracy: 0.9584 - val_loss: 1.5943 - val_accuracy: 0.6339
Epoch 67/70
1563/1563 [==============================] - 133s 85ms/step - loss: 0.1531 - accuracy: 0.9610 - val_loss: 1.6097 - val_accuracy: 0.6371
Epoch 68/70
1563/1563 [==============================] - 126s 81ms/step - loss: 0.1454 - accuracy: 0.9637 - val_loss: 1.6421 - val_accuracy: 0.6276
Epoch 69/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.1361 - accuracy: 0.9676 - val_loss: 1.6738 - val_accuracy: 0.6331
Epoch 70/70
1563/1563 [==============================] - 131s 84ms/step - loss: 0.1294 - accuracy: 0.9702 - val_loss: 1.7189 - val_accuracy: 0.6208
63/63 - 3s - loss: 1.8016 - accuracy: 0.6080

FENOTIPO: (((conv*2)pool)*3)fc*2

Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_3/MaxPool}} = MaxPool[T=DT_FLOAT, data_format="NHWC", ksize=[1, 2, 2, 1], padding="VALID", strides=[1, 2, 2, 1]](conv2d_6/Identity)' with input shapes: [?,1,1,128].

FENOTIPO: (((conv*3)pool)*3)fc*1

Negative dimension size caused by subtracting 3 from 1 for '{{node conv2d_14/Conv2D}} = Conv2D[T=DT_FLOAT, data_format="NHWC", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_13/Identity, conv2d_14/Conv2D/ReadVariableOp)' with input shapes: [?,1,1,256], [3,3,256,256].

FENOTIPO: (((conv*1)pool)*3)fc*0

Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_15 (Conv2D)           (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 13, 13, 32)        9248      
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 4, 4, 64)          18496     
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 2, 2, 64)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
=================================================================
Total params: 31,210
Trainable params: 31,210
Non-trainable params: 0
_________________________________________________________________
Epoch 1/70
1563/1563 [==============================] - 72s 46ms/step - loss: 2.1059 - accuracy: 0.2214 - val_loss: 1.9620 - val_accuracy: 0.2916
Epoch 2/70
1563/1563 [==============================] - 69s 44ms/step - loss: 1.8321 - accuracy: 0.3408 - val_loss: 2.1061 - val_accuracy: 0.2734
Epoch 3/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.6730 - accuracy: 0.3965 - val_loss: 1.6152 - val_accuracy: 0.4157
Epoch 4/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.5883 - accuracy: 0.4306 - val_loss: 1.6086 - val_accuracy: 0.4207
Epoch 5/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.5258 - accuracy: 0.4553 - val_loss: 1.5229 - val_accuracy: 0.4490
Epoch 6/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.4792 - accuracy: 0.4715 - val_loss: 1.4705 - val_accuracy: 0.4670
Epoch 7/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.4400 - accuracy: 0.4897 - val_loss: 1.4852 - val_accuracy: 0.4736
Epoch 8/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.4076 - accuracy: 0.4991 - val_loss: 1.4642 - val_accuracy: 0.4807
Epoch 9/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.3803 - accuracy: 0.5117 - val_loss: 1.3830 - val_accuracy: 0.5089
Epoch 10/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.3565 - accuracy: 0.5231 - val_loss: 1.3938 - val_accuracy: 0.5035
Epoch 11/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.3386 - accuracy: 0.5266 - val_loss: 1.3795 - val_accuracy: 0.5110
Epoch 12/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.3222 - accuracy: 0.5360 - val_loss: 1.3899 - val_accuracy: 0.5115
Epoch 13/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.3075 - accuracy: 0.5419 - val_loss: 1.3109 - val_accuracy: 0.5425
Epoch 14/70
1563/1563 [==============================] - 65s 41ms/step - loss: 1.2952 - accuracy: 0.5417 - val_loss: 1.4240 - val_accuracy: 0.5073
Epoch 15/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.2864 - accuracy: 0.5477 - val_loss: 1.3205 - val_accuracy: 0.5303
Epoch 16/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.2726 - accuracy: 0.5547 - val_loss: 1.3657 - val_accuracy: 0.5186
Epoch 17/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.2623 - accuracy: 0.5566 - val_loss: 1.3824 - val_accuracy: 0.5286
Epoch 18/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.2511 - accuracy: 0.5605 - val_loss: 1.3533 - val_accuracy: 0.5285
Epoch 19/70
1563/1563 [==============================] - 68s 44ms/step - loss: 1.2420 - accuracy: 0.5645 - val_loss: 1.3583 - val_accuracy: 0.5238
Epoch 20/70
1563/1563 [==============================] - 69s 44ms/step - loss: 1.2338 - accuracy: 0.5666 - val_loss: 1.2856 - val_accuracy: 0.5551
Epoch 21/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.2289 - accuracy: 0.5707 - val_loss: 1.3029 - val_accuracy: 0.5431
Epoch 22/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.2183 - accuracy: 0.5731 - val_loss: 1.2664 - val_accuracy: 0.5602
Epoch 23/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.2109 - accuracy: 0.5780 - val_loss: 1.4805 - val_accuracy: 0.4935
Epoch 24/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.2039 - accuracy: 0.5793 - val_loss: 1.2611 - val_accuracy: 0.5574
Epoch 25/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1990 - accuracy: 0.5825 - val_loss: 1.2553 - val_accuracy: 0.5744
Epoch 26/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.1875 - accuracy: 0.5844 - val_loss: 1.3346 - val_accuracy: 0.5441
Epoch 27/70
1563/1563 [==============================] - 65s 42ms/step - loss: 1.1822 - accuracy: 0.5879 - val_loss: 1.2527 - val_accuracy: 0.5688
Epoch 28/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1808 - accuracy: 0.5881 - val_loss: 1.2589 - val_accuracy: 0.5671
Epoch 29/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.1705 - accuracy: 0.5931 - val_loss: 1.2973 - val_accuracy: 0.5554
Epoch 30/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1689 - accuracy: 0.5934 - val_loss: 1.2189 - val_accuracy: 0.5748
Epoch 31/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1580 - accuracy: 0.5981 - val_loss: 1.2802 - val_accuracy: 0.5535
Epoch 32/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.1585 - accuracy: 0.5981 - val_loss: 1.2347 - val_accuracy: 0.5719
Epoch 33/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1496 - accuracy: 0.5996 - val_loss: 1.2398 - val_accuracy: 0.5686
Epoch 34/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.1458 - accuracy: 0.6005 - val_loss: 1.2609 - val_accuracy: 0.5680
Epoch 35/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1379 - accuracy: 0.6029 - val_loss: 1.2331 - val_accuracy: 0.5759
Epoch 36/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1384 - accuracy: 0.6071 - val_loss: 1.2317 - val_accuracy: 0.5776
Epoch 37/70
1563/1563 [==============================] - 65s 42ms/step - loss: 1.1307 - accuracy: 0.6075 - val_loss: 1.1762 - val_accuracy: 0.5921
Epoch 38/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.1255 - accuracy: 0.6102 - val_loss: 1.2632 - val_accuracy: 0.5664
Epoch 39/70
1563/1563 [==============================] - 65s 42ms/step - loss: 1.1187 - accuracy: 0.6123 - val_loss: 1.1685 - val_accuracy: 0.5995
Epoch 40/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.1124 - accuracy: 0.6144 - val_loss: 1.2403 - val_accuracy: 0.5686
Epoch 41/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.1032 - accuracy: 0.6181 - val_loss: 1.3281 - val_accuracy: 0.5535
Epoch 42/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.1035 - accuracy: 0.6194 - val_loss: 1.2402 - val_accuracy: 0.5700
Epoch 43/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0970 - accuracy: 0.6202 - val_loss: 1.2259 - val_accuracy: 0.5814
Epoch 44/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0963 - accuracy: 0.6213 - val_loss: 1.2033 - val_accuracy: 0.5903
Epoch 45/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0871 - accuracy: 0.6248 - val_loss: 1.2613 - val_accuracy: 0.5742
Epoch 46/70
1563/1563 [==============================] - 68s 44ms/step - loss: 1.0856 - accuracy: 0.6249 - val_loss: 1.1729 - val_accuracy: 0.5975
Epoch 47/70
1563/1563 [==============================] - 66s 43ms/step - loss: 1.0807 - accuracy: 0.6256 - val_loss: 1.1716 - val_accuracy: 0.6040
Epoch 48/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.0814 - accuracy: 0.6276 - val_loss: 1.1550 - val_accuracy: 0.6087
Epoch 49/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0761 - accuracy: 0.6291 - val_loss: 1.2166 - val_accuracy: 0.5856
Epoch 50/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0738 - accuracy: 0.6305 - val_loss: 1.1988 - val_accuracy: 0.5897
Epoch 51/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0694 - accuracy: 0.6308 - val_loss: 1.2222 - val_accuracy: 0.5878
Epoch 52/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0651 - accuracy: 0.6323 - val_loss: 1.2770 - val_accuracy: 0.5725
Epoch 53/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0649 - accuracy: 0.6342 - val_loss: 1.1578 - val_accuracy: 0.6043
Epoch 54/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0597 - accuracy: 0.6358 - val_loss: 1.1424 - val_accuracy: 0.6099
Epoch 55/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0576 - accuracy: 0.6342 - val_loss: 1.1773 - val_accuracy: 0.5950
Epoch 56/70
1563/1563 [==============================] - 69s 44ms/step - loss: 1.0540 - accuracy: 0.6357 - val_loss: 1.1853 - val_accuracy: 0.6051
Epoch 57/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0560 - accuracy: 0.6354 - val_loss: 1.2240 - val_accuracy: 0.5874
Epoch 58/70
1563/1563 [==============================] - 65s 41ms/step - loss: 1.0497 - accuracy: 0.6368 - val_loss: 1.1800 - val_accuracy: 0.6012
Epoch 59/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0484 - accuracy: 0.6392 - val_loss: 1.1645 - val_accuracy: 0.6030
Epoch 60/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0453 - accuracy: 0.6384 - val_loss: 1.2480 - val_accuracy: 0.5866
Epoch 61/70
1563/1563 [==============================] - 68s 43ms/step - loss: 1.0451 - accuracy: 0.6419 - val_loss: 1.2703 - val_accuracy: 0.5869
Epoch 62/70
1563/1563 [==============================] - 68s 44ms/step - loss: 1.0490 - accuracy: 0.6400 - val_loss: 1.2579 - val_accuracy: 0.5910
Epoch 63/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0433 - accuracy: 0.6402 - val_loss: 1.2075 - val_accuracy: 0.5934
Epoch 64/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0399 - accuracy: 0.6436 - val_loss: 1.1505 - val_accuracy: 0.6119
Epoch 65/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0397 - accuracy: 0.6429 - val_loss: 1.1782 - val_accuracy: 0.6124
Epoch 66/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0383 - accuracy: 0.6435 - val_loss: 1.2307 - val_accuracy: 0.5947
Epoch 67/70
1563/1563 [==============================] - 69s 44ms/step - loss: 1.0394 - accuracy: 0.6449 - val_loss: 1.1839 - val_accuracy: 0.6072
Epoch 68/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0339 - accuracy: 0.6462 - val_loss: 1.1901 - val_accuracy: 0.6084
Epoch 69/70
1563/1563 [==============================] - 66s 42ms/step - loss: 1.0356 - accuracy: 0.6474 - val_loss: 1.1364 - val_accuracy: 0.6291
Epoch 70/70
1563/1563 [==============================] - 67s 43ms/step - loss: 1.0323 - accuracy: 0.6480 - val_loss: 1.1765 - val_accuracy: 0.6074
63/63 - 1s - loss: 1.1838 - accuracy: 0.6005

FENOTIPO: (((conv*3))*3)fc*0

Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_18 (Conv2D)           (None, 30, 30, 32)        896       
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 28, 28, 32)        9248      
_________________________________________________________________
conv2d_20 (Conv2D)           (None, 26, 26, 64)        18496     
_________________________________________________________________
conv2d_21 (Conv2D)           (None, 24, 24, 64)        36928     
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 22, 22, 128)       73856     
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 20, 20, 128)       147584    
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 18, 18, 256)       295168    
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 16, 16, 256)       590080    
_________________________________________________________________
conv2d_26 (Conv2D)           (None, 14, 14, 512)       1180160   
_________________________________________________________________
flatten_2 (Flatten)          (None, 100352)            0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1003530   
=================================================================
Total params: 3,355,946
Trainable params: 3,355,946
Non-trainable params: 0
_________________________________________________________________
Epoch 1/70
1563/1563 [==============================] - 3909s 3s/step - loss: 2.0239 - accuracy: 0.2586 - val_loss: 1.9523 - val_accuracy: 0.2905
Epoch 2/70
1563/1563 [==============================] - 3903s 2s/step - loss: 1.6955 - accuracy: 0.3857 - val_loss: 1.5660 - val_accuracy: 0.4289
Epoch 3/70
1563/1563 [==============================] - 3906s 2s/step - loss: 1.5152 - accuracy: 0.4511 - val_loss: 1.5942 - val_accuracy: 0.4146
Epoch 4/70
1563/1563 [==============================] - 3916s 3s/step - loss: 1.3886 - accuracy: 0.5017 - val_loss: 1.3523 - val_accuracy: 0.5121
Epoch 5/70
1563/1563 [==============================] - 3910s 3s/step - loss: 1.2825 - accuracy: 0.5451 - val_loss: 1.5809 - val_accuracy: 0.4651
Epoch 6/70
1563/1563 [==============================] - 3907s 2s/step - loss: 1.1853 - accuracy: 0.5842 - val_loss: 1.3418 - val_accuracy: 0.5343
Epoch 7/70
1563/1563 [==============================] - 3875s 2s/step - loss: 1.0855 - accuracy: 0.6191 - val_loss: 1.2786 - val_accuracy: 0.5655
Epoch 8/70
1563/1563 [==============================] - 3899s 2s/step - loss: 0.9991 - accuracy: 0.6505 - val_loss: 1.1757 - val_accuracy: 0.5854
Epoch 9/70
 742/1563 [=============>................] - ETA: 32:12 - loss: 0.9048 - accuracy: 0.6806^CTraceback (most recent call last):
  File "ponyge.py", line 31, in <module>
    mane()
  File "ponyge.py", line 23, in mane
    individuals = params['SEARCH_LOOP']()
  File "/home/cleber/workspace/PonyGE2/src/algorithm/search_loop.py", line 27, in search_loop
    individuals = evaluate_fitness(individuals)
  File "/home/cleber/workspace/PonyGE2/src/fitness/evaluation.py", line 76, in evaluate_fitness
    results = eval_or_append(ind, results, pool)
  File "/home/cleber/workspace/PonyGE2/src/fitness/evaluation.py", line 118, in eval_or_append
    ind.evaluate()
  File "/home/cleber/workspace/PonyGE2/src/representation/individual.py", line 122, in evaluate
    self.fitness = params['FITNESS_FUNCTION'](self)
  File "/home/cleber/workspace/PonyGE2/src/fitness/base_ff_classes/base_ff.py", line 35, in __call__
    fitness = self.evaluate(ind, **kwargs)
  File "/home/cleber/workspace/PonyGE2/src/fitness/paper.py", line 80, in evaluate
    model.fit(train_images, train_labels, epochs=70, validation_data=(validation_images, validation_labels))
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/home/cleber/softwares/miniconda3/envs/ge/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
